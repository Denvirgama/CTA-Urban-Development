---
title: "Update 2"
author: "Zach Hollis"
date: "2024-05-10"
output: html_document
---

```{r setup, include=FALSE}
library(tigris)
library(ggplot2)
library(lubridate)
library(dplyr)
library(tidyr)
library(sf)
library(ggplot2)
library(openxlsx)
library(tidycensus)
library(caret)
```
```{r}
Census_Data_Cleaned <- "C:\\Users\\holli\\Desktop\\Depaul\\Spring 2024\\Capstone\\Cleaned Data Set\\Cleaned_Merged_Data.xlsx"
Census_Data_Cleaned <- read.xlsx(Census_Data_Cleaned)

DF_Census_Data_Cleaned <- as.data.frame(Census_Data_Cleaned)

DF_Census_Data_Cleaned$Geography <- sub(".*US", "", DF_Census_Data_Cleaned$Geography)

DF_Census_Data_Cleaned <- DF_Census_Data_Cleaned %>% rename(GEOID = Geography)


DF_Census_Data_Cleaned

```

```{r}
#Model to predict Public.transportation
#Remove Geographic codes as they are not actual data for the model
model <- lm(Public.transportation ~ . - GEOID, data = DF_Census_Data_Cleaned)

summary(model)

# plots for the model
par(mfrow = c(2, 2))
plot(model)
```

```{r}
#summary(model)

#Extract Significant Variables
coefficients_summary <- summary(model)$coefficients
significant_vars <- coefficients_summary[coefficients_summary[, "Pr(>|t|)"] < 0.05, ]
print(significant_vars)


#Based on the initial model we can see that the following variables are most significant 
```

```{r}
#create list of significant variables for new lm model
significant_var_names <- rownames(significant_vars)
significant_var_names <- significant_var_names[significant_var_names != "(Intercept)"]
formula_str <- paste("Public.transportation ~", paste(significant_var_names, collapse = " + "))
model_formula <- as.formula(formula_str)

model_top_attributes <- lm(model_formula, data = DF_Census_Data_Cleaned)

summary(model_top_attributes)

#Using only significant variables we can see that there is a minimal drop in adjusted R^2
```




```{r}

set.seed(123)  # Setting seed for reproducibility
trainIndex <- createDataPartition(DF_Census_Data_Cleaned$Public.transportation, p = 0.80, list = FALSE)

# Create the training and testing datasets
trainData <- DF_Census_Data_Cleaned[trainIndex, ]
testData <- DF_Census_Data_Cleaned[-trainIndex, ]

# Now you can fit a linear model on the training data
model_train <- lm(model_formula, data = trainData)

# Summary of the model
summary(model_train)


```

```{r}
# Predict on the test data
predictions <- predict(model_train, newdata = testData)

# Actual values from the test data
actuals <- testData$Public.transportation
```



```{r}
# Residuals
residuals <- actuals - predictions
#residuals

```
```{r}
# Calculate Mean Squared Error and Root Mean Squared Error 
mse <- mean(residuals^2)
rmse <- sqrt(mse)

#Mean Absolute Error (MAE)
mae <- mean(abs(residuals))

#R-squared value
sse <- sum(residuals^2)
sst <- sum((actuals - mean(actuals))^2)
r_squared <- 1 - (sse/sst)

cat("Mean Squared Error:", mse, "\n")
cat("Root Mean Squared Error:", rmse, "\n")
cat("Mean Absolute Error:", mae, "\n")
cat("R-squared:", r_squared, "\n")
```


```{r}
library(ggplot2)

# Prediction vs Actuals Scatter Plot
ggplot(data = testData, aes(x = actuals, y = predictions)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = lm, col = "red") +
  labs(x = "Actual Values", y = "Predicted Values", title = "Actual vs. Predicted Plot") +
  theme_minimal()
```

```{r}
# Residuals Plot
ggplot(data = testData, aes(x = predictions, y = residuals)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Predicted Values", y = "Residuals", title = "Residuals Plot") +
  theme_minimal()
```


```{r}
L_Station_coords <- "C:\\Users\\holli\\Desktop\\Depaul\\Spring 2024\\Capstone\\Cleaned Data Set\\L Station Coords with Station ID.csv"
L_Station_coords <- read.csv(L_Station_coords)

options(tigris_class = "sf")

# Convert dataframe to sf object
coords_sf <- st_as_sf(L_Station_coords, coords = c("X", "Y"), crs = 4326, agr = "constant")

# Fetch census tracts for Illinois
tracts <- tracts(state = "IL", cb = TRUE, year = 2020)

# CRS - Needs to be done to ensure correct coordinate types are being used 
tracts <- st_transform(tracts, crs = st_crs(coords_sf))

#spatial join
tracts_info <- st_join(coords_sf, tracts)


results <- tracts_info %>%
  select(STATION_DESCRIPTIVE_NAME, GEOID, station_id, geometry)


StationGEOID <- as.data.frame(results)
StationGEOID$geometry <- NULL


file_path <- "C:\\Users\\holli\\Desktop\\Depaul\\Spring 2024\\Capstone\\StationGEOID"
file_path2 <- "C:\\Users\\holli\\Desktop\\Depaul\\Spring 2024\\Capstone\\Cleaned Merged Census Data"

write.xlsx(StationGEOID, file_path)
write.xlsx(DF_Census_Data_Cleaned, file_path2)
```
```{r}
# Perform the merge (inner join)
merged_data <- inner_join(DF_Census_Data_Cleaned, StationGEOID, by = "GEOID")

# View the first few rows of the merged data
head(merged_data)
```

```{r}
Daily_Entries_CTA_Stations <- "C:\\Users\\holli\\Desktop\\Depaul\\Spring 2024\\Capstone\\CTA_L_Entries_Daily_Totals.csv"

Daily_Entries_CTA_Stations <- read.csv(Daily_Entries_CTA_Stations)

Daily_Entries_CTA_Stations <- Daily_Entries_CTA_Stations %>%
  separate(date, into = c("Month", "Day", "Year"), sep = "/", convert = TRUE)

#file_path <- "C:\\Users\\holli\\Desktop\\Depaul\\Spring 2024\\Capstone\\CTA Station Entries Data Set"
#write.xlsx(Daily_Entries_CTA_Stations, file_path)
```




```{R}
#Average Daily Entries for each station in 2023 for each day type: #W=Weekday, A=Saturday, U=Sunday/Holiday

CTA_entries_2023 <- filter(Daily_Entries_CTA_Stations, Year == 2023)

average_entries <- CTA_entries_2023 %>%
  group_by(stationname, station_id, daytype) %>%
  summarise(average_daily_entries = mean(rides, na.rm = TRUE))

print(average_entries)
```

```{r}
Daily_Entries_By_Station_DayType <- average_entries %>%
  pivot_wider(
    names_from = daytype,  
    values_from = average_daily_entries
  )

# Print the wide format data to check the results
print(Daily_Entries_By_Station_DayType)
```

```{r}
merged_data <- inner_join(merged_data, Daily_Entries_By_Station_DayType, by = "station_id")

CTA_Final_Data_Set <- merged_data
file_path <- "C:\\Users\\holli\\Desktop\\Depaul\\Spring 2024\\Capstone\\Cleaned Data Set\\CTA_Final_Data_Set"
write.xlsx(CTA_Final_Data_Set, file_path)
```


```{r}
#W=Weekday, A=Saturday, U=Sunday/Holiday

#Take out names/GEOID and average daily entries for non Saturday 
model_A <- lm(A ~ . - GEOID - STATION_DESCRIPTIVE_NAME - stationname -station_id - U - W, data = CTA_Final_Data_Set)

summary(model_A)

```

```{r}
model_U <- lm(U ~ . - GEOID - STATION_DESCRIPTIVE_NAME - stationname -station_id - A - W, data = CTA_Final_Data_Set)
summary(model_U)
```

```{r}
model_W <- lm(W ~ . - GEOID - STATION_DESCRIPTIVE_NAME - stationname -station_id - A - U, data = CTA_Final_Data_Set)
summary(model_W)
```
```{r}
library(caret)
library(MASS)
library(car)
library(e1071)

features <- CTA_Final_Data_Set[, !(names(CTA_Final_Data_Set) %in% c("A", "U", "W", "GEOID", "STATION_DESCRIPTIVE_NAME", "station_id", "stationname"))]
target_A <- CTA_Final_Data_Set$A





```

```{r}
#Train Test Split
set.seed(123)
trainIndex <- createDataPartition(target_A, p = .8, 
                                  list = FALSE, 
                                  times = 1)
dataTrain <- features[trainIndex,]
dataTest <- features[-trainIndex,]
targetTrain <- target_A[trainIndex]
targetTest <- target_A[-trainIndex]
```

```{r}
# Train the SVM model 1
svm_model <- svm(dataTrain, targetTrain)

# Make predictions on the test set
predictions <- predict(svm_model, dataTest)

# Evaluate the model
results <- postResample(predictions, targetTest)
print(results)
```
```{r}
#Non radial svm with grid search for tuning
trainData <- dataTrain
trainData$U <- targetTrain

# Define the training control
train_control <- trainControl(method = "cv", number = 10)

# Define the parameter grid for hyperparameter tuning
tune_grid <- expand.grid(sigma = 2^(-15:3), C = 2^(-5:15))

# Train model 
svm_model_radial <- train(U ~ ., data = trainData, method = "svmRadial",
                   trControl = train_control, tuneGrid = tune_grid)


print(svm_model_radial$bestTune)
```
```{r}
predictions_radial <- predict(svm_model_radial, dataTest)

# Evaluate the model
results <- postResample(predictions_radial, targetTest)
print(results)
```
```{r}
#poly svm with grid search for tuning
trainData <- dataTrain
trainData$U <- targetTrain

# Define the training control
train_control <- trainControl(method = "cv", number = 10)

# Define the parameter grid for hyperparameter tuning
tune_grid <- expand.grid(degree = 1:3, scale = c(0.1, 1, 10), C = 2^(1:5))

# Train model polynomial 
svm_model_poly <- train(U ~ ., data = trainData, method = "svmPoly",
                   trControl = train_control, tuneGrid = tune_grid)


print(svm_model_poly$bestTune)

predictions_poly <- predict(svm_model_poly, dataTest)

# Evaluate the model
results <- postResample(predictions_poly, targetTest)
print(results)
```

```{R}
library(rpart)
library(rpart.plot)
train_control <- trainControl(method = "cv", number = 10)

# Define the parameter grid for hyperparameter tuning
ttune_grid <- expand.grid(cp = seq(0.001, 0.1, by = 0.001),
                         maxdepth = c(1, 3, 5, 7, 9),
                         minsplit = c(1, 5, 10, 20))

# Train the decision tree model with cross-validation
tree_model <- train(U ~ ., data = trainData, method = "rpart",
                    trControl = train_control, tuneGrid = tune_grid)

# Print the best model parameters
print(tree_model$bestTune)

# Plot the decision tree
rpart.plot(tree_model$finalModel)

# Make predictions on the test set
predictions <- predict(tree_model, dataTest)

# Evaluate the model
results <- postResample(predictions, targetTest)
print(results)
```
```{r}
#decision tree with binned data
target <- CTA_Final_Data_Set$U
bins <- quantile(target, probs = seq(0, 1, by = 0.2), na.rm = TRUE)
labels <- c("very low", "low", "medium", "high", "very high")
target_bins <- cut(target, breaks = bins, labels = labels, include.lowest = TRUE)

# Combine the features and the binned target variable
data_U_days <- cbind(features, U_bin = target_bins)

#redo test and train split
set.seed(123)
trainIndex <- createDataPartition(data_U_days$U_bin, p = .8, 
                                  list = FALSE, 
                                  times = 1)
dataTrain <- data_U_days[trainIndex,]
dataTest <- data_U_days[-trainIndex,]


tree_model <- train(U_bin ~ ., data = dataTrain, method = "rpart",
                    trControl = train_control, tuneGrid = tune_grid)

# Print the best model parameters
print(tree_model$bestTune)

# Plot the decision tree
rpart.plot(tree_model$finalModel)
```
```{r}
# Make predictions on the test set
predictions <- predict(tree_model, dataTest)

# Evaluate the model
confusionMatrix(predictions, dataTest$U_bin)


```
```{R}
# Define the training control
train_control <- trainControl(method = "cv", number = 10)

# Define the parameter grid for hyperparameter tuning
tune_grid <- expand.grid(k = seq(3, 21, by = 2))  # k values from 3 to 21

# Train the kNN model with cross-validation
knn_model <- train(U_bin ~ ., data = dataTrain, method = "knn",
                   trControl = train_control, tuneGrid = tune_grid)

# Print the best model parameters
print(knn_model$bestTune)

```
```{r}
predictions <- predict(knn_model, dataTest)

# Evaluate the model
results <- postResample(predictions, dataTest$U_bin)
print(results)

plot(knn_model)
```

```{r}
library(cluster)
library(FactoMineR)
set.seed(123)
num_clusters <- 5  # Set the number of clusters
kmeans_result <- kmeans(scaledFeatures, centers = num_clusters, nstart = 25)

pca_result <- PCA(scaledFeatures, graph = FALSE)

# Create a data frame with PCA results and cluster assignments
pca_result <- PCA(scaledFeatures, graph = FALSE)

# Create a data frame with PCA results, cluster assignments, and U_bins
pca_data <- data.frame(pca_result$ind$coord)
pca_data$Cluster <- factor(kmeans_result$cluster)
pca_data$U_bins <- target_bins


# Plot the PCA results with clusters and U_bins labels
ggplot(pca_data, aes(x = Dim.1, y = Dim.2, color = Cluster, label = U_bins)) +
  geom_point(size = 2, alpha = 0.6) +
  geom_text(vjust = 1.5, size = 3) +  # Add labels
  labs(title = "PCA - k-means Clustering with U_bins Labels", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal() +
  scale_color_discrete(name = "Cluster")

```
```{r}
pca_summary <- summary(pca_result)
print(pca_summary)



```